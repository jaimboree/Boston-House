{"paragraphs":[{"text":"%pyspark\nfrom pyspark.mllib.feature import StandardScaler,StandardScalerModel\nfrom pyspark.mllib.regression import LabeledPoint, LinearRegressionWithSGD, LinearRegressionModel\n\nhouses = sc.textFile('hdfs://sandbox.hortonworks.com/tmp/boston-data/boston_house.csv')\nheader = houses.first()\nheaderless_houses = houses.filter(lambda line: line != header)\nheaderless_houses = headerless_houses.map(lambda x: x.split(\",\"))\n\nhouses_df = headerless_houses.map(lambda x: Row(CRIM = x[0], ZN = x[1], INDUS = x[2], CHAS = x[3], NOX = x[4], RM = x[5], AGE = x[6], DIS = x[7], RAD = x[8], TAX = x[9], PTRATIO = x[10], B = x[11], LSTAT = x[12])).toDF()\n\nfeatures = houses_df.map(lambda row: row[0:])\n\nscaler = StandardScaler(withMean=True, withStd=True).fit(features)\n\ndatapoints = scaler.transform(features)\nlabel = headerless_houses.map(lambda x: x[13])\nlabeledDataPoints = label.zip(datapoints)\nlabeledDataPoints = labeledDataPoints.map(lambda x: LabeledPoint(x[0], x[1]))\nmodel = LinearRegressionWithSGD.train(labeledDataPoints, intercept=True)\n\n \ntrainingdata = sc.textFile('hdfs://sandbox.hortonworks.com/tmp/boston-data/verification.csv')\nheader = trainingdata.first()\nheaderless_training = trainingdata.filter(lambda line: line != header)\nheaderless_training = headerless_training.map(lambda x: x.split(\",\"))\ntraining_df = headerless_training.map(lambda x: Row(CRIM = x[0], ZN = x[1], INDUS = x[2], CHAS = x[3], NOX = x[4], RM = x[5], AGE = x[6], DIS = x[7], RAD = x[8], TAX = x[9], PTRATIO = x[10], B = x[11], LSTAT = x[12])).toDF()\ntrainingfeatures = training_df.map(lambda row: row[0:])\ntraining_scaled = scaler.transform(trainingfeatures)\nverify_predictions = model.predict(training_scaled)\n\n#print verify_predictions.take(3)\nverify_predictions.saveAsTextFile('hdfs://sandbox.hortonworks.com/tmp/boston-data/predicted_results')","dateUpdated":"2016-09-29T06:58:55+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1475122108906_-322342979","id":"20160929-040828_837804905","dateCreated":"2016-09-29T04:08:28+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3779","dateFinished":"2016-09-29T06:59:00+0000","dateStarted":"2016-09-29T06:58:55+0000","result":{"code":"SUCCESS","type":"TEXT","msg":""},"focus":true},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1475128647029_2014936472","id":"20160929-055727_356527980","dateCreated":"2016-09-29T05:57:27+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3886","dateUpdated":"2016-09-29T06:10:33+0000","dateFinished":"2016-09-29T06:02:05+0000","dateStarted":"2016-09-29T06:02:05+0000","result":{"code":"SUCCESS","type":"TEXT","msg":""},"text":"\ndatapoints = scaler.transform(features)\nlabel = house_df.select('MDEV')\nlabeledDataPoints = label.zip(datapoints)\nlabeledDataPoints = labeledDataPoints.map(lambda x: LabledPoint(x[0], x[1]))\n \n \n\n \n \ntrainingdata = sc.textFile()\ntraining_scale = scalar.trasform(trainingdata)\n \nverify_predictions = model.predict(training_scale)\n \nverify_predictions.saveAsTextFile('predicted_results');\nprint(\"\\n\\n\\n------------------------------------------------------------------------------\\n\")\n\n\nprint(features.show(5))\n\n\nprint(\"\\n------------------------------------------------------------------------------\\n\\n\\n\")\n\n"}],"name":"Group-#1-Boston-House","id":"2BWSHVX67","angularObjects":{"2BW4S5NJ4:shared_process":[],"2BY1Y2HJG:shared_process":[],"2BVYH21ZJ:shared_process":[],"2BV6637D6:shared_process":[],"2BX5CPDSX:shared_process":[],"2BWHGGFZJ:shared_process":[]},"config":{"looknfeel":"default"},"info":{}}